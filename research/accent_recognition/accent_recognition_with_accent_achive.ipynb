{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"accent_recognition_with_accent_achive.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"124FS6CuCuibdh0RNZjEUMyrSEAwyuMNA","authorship_tag":"ABX9TyPau1dnFTV1Dwy3xtM8hqQW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"73e52f83ae9e4e67b13b878d4a02e7e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c2b0638d3054bc2bcdaec18c90bd7e6","IPY_MODEL_809c573f825e4e2cbe0b930a0d4c562c","IPY_MODEL_73129ce29adc4581a7c20363aa50424a"],"layout":"IPY_MODEL_abdc0468edc6432d96ff6e34f3351c39"}},"3c2b0638d3054bc2bcdaec18c90bd7e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8401911193e640cc8f2d9626ecb6240b","placeholder":"​","style":"IPY_MODEL_655d2d2300c246d48631eedcfe50f64e","value":"100%"}},"809c573f825e4e2cbe0b930a0d4c562c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77ec3edb664849b19ea4aface52b2666","max":46830571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e5e3942f4fd4779a532d17ecffb4236","value":46830571}},"73129ce29adc4581a7c20363aa50424a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48257e9a56654b39a2ecf4ae417e6686","placeholder":"​","style":"IPY_MODEL_03ff015243904ec380b84ebb44651065","value":" 44.7M/44.7M [00:00&lt;00:00, 171MB/s]"}},"abdc0468edc6432d96ff6e34f3351c39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8401911193e640cc8f2d9626ecb6240b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655d2d2300c246d48631eedcfe50f64e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77ec3edb664849b19ea4aface52b2666":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5e3942f4fd4779a532d17ecffb4236":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48257e9a56654b39a2ecf4ae417e6686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03ff015243904ec380b84ebb44651065":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fURJ4rWZrcsN","executionInfo":{"status":"ok","timestamp":1652917625743,"user_tz":-180,"elapsed":3165,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"outputs":[],"source":["import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import time\n","import sys\n","import re\n","import os\n","from os import listdir\n","import matplotlib.pyplot as plt\n","from os.path import isfile, join\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","import librosa\n","import librosa.display\n","\n","import cv2\n","import zipfile\n","from PIL import Image\n","\n","import time\n","import copy"]},{"cell_type":"markdown","source":["# Get accent archive"],"metadata":{"id":"0qMdHyA3tGZt"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NIS_2022/accent_archive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGlbWQEgskVq","executionInfo":{"status":"ok","timestamp":1652304487220,"user_tz":-180,"elapsed":231,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"96757cc5-a44e-45c2-9a94-d19873ff45b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NIS_2022/accent_archive\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/akshanshchaudhry/Speech-Accent-Recognition.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xa5D3Zs0uuUK","executionInfo":{"status":"ok","timestamp":1652299664209,"user_tz":-180,"elapsed":1575,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"fea68b80-ab25-4147-aa37-5b7d9879ce9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Speech-Accent-Recognition'...\n","remote: Enumerating objects: 21, done.\u001b[K\n","remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n","Unpacking objects: 100% (21/21), done.\n"]}]},{"cell_type":"code","source":["!pip install pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2YCf4AB-RED","executionInfo":{"status":"ok","timestamp":1652303707343,"user_tz":-180,"elapsed":4253,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"595609c4-4f71-4fe5-c009-3f27375b3f81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}]},{"cell_type":"code","source":["%cd ../Speech-Accent-Recognition/src"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pRod64nNvAPF","executionInfo":{"status":"ok","timestamp":1652307944317,"user_tz":-180,"elapsed":231,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"3d326926-a9ee-4380-e665-57d47ee33b9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NIS_2022/accent_archive/Speech-Accent-Recognition/src\n"]}]},{"cell_type":"code","source":["!python fromwebsite.py /content/drive/MyDrive/NIS_2022/accent_archive/data/bio_metadata.csv  chinese english russian japanese arabic spanish korean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sbu4KwUrvIjZ","executionInfo":{"status":"ok","timestamp":1652307292681,"user_tz":-180,"elapsed":64655,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"ef27687d-d86b-488f-8889-e0a107dbda78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading from http://accent.gmu.edu/browse_language.php?function=find&language=japanese\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=221\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=222\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=223\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=224\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=225\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=226\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=227\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=486\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=543\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=827\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1046\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1365\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1381\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1382\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1521\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1537\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1610\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1626\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1684\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1833\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1847\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1881\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1942\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1948\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1987\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=1991\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2123\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2390\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2436\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2492\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2493\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2547\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2555\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2567\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2723\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2797\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2807\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2826\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2843\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2850\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2866\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2940\n","downloading from http://accent.gmu.edu/browse_language.php?function=detail&speakerid=2953\n","loading finished\n"]}]},{"cell_type":"code","source":["df = pd.read_csv ('/content/drive/MyDrive/NIS_2022/accent_archive/data/processed_bio_metadata.csv',\n","                  sep='\\t')"],"metadata":{"id":"NaggCqcpGUBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["categories = []\n","for index, row in df.iterrows():\n","    category = 'default'\n","    if 'china' in row['birth_place'] or 'taiwan' in row['birth_place']:\n","        category = 'chinese'\n","    elif 'english' in row['language_num']:\n","        if 'usa' in row['birth_place']:\n","            category = 'usa'\n","        elif 'uk' in row['birth_place']:\n","            category = 'uk'\n","        elif 'canada' in row['birth_place']:\n","            category = 'canada'\n","        elif 'australia' in row['birth_place']:\n","            category = 'australia'\n","    elif 'russian' in row['language_num']:\n","        category = 'russian'\n","    elif 'arabic' in row['language_num']:\n","        category = 'arabic'\n","    elif 'spanish' in row['language_num']:\n","        category = 'spanish'\n","    elif 'korean' in row['language_num']:\n","        category = 'korean'\n","    elif 'japanese' in row['language_num']:\n","        category = 'japanese'\n","    categories.append(category)\n","    "],"metadata":{"id":"zzS02fvxGZN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['category'] = categories"],"metadata":{"id":"a8g8GgRxMrrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(categories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXVudKZcQ69f","executionInfo":{"status":"ok","timestamp":1652309036200,"user_tz":-180,"elapsed":221,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"e19ddbcb-3920-4a4b-b878-8438d550b239"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1521"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["df.to_csv(r'/content/drive/MyDrive/NIS_2022/accent_archive/data/processed_bio_metadata.csv', index=False, sep='\\t', header='true')"],"metadata":{"id":"UNQgumH5M7ie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python getaudio.py /content/drive/MyDrive/NIS_2022/accent_archive/data/processed_bio_metadata.csv"],"metadata":{"id":"2y9IzcHcvORp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset create"],"metadata":{"id":"MmH7lUMyYNh_"}},{"cell_type":"code","source":["def get_melspectrogram_db(file_path, sr=None, n_fft=2048, hop_length=512, n_mels=128, fmin=20, fmax=8300, top_db=80):\n","    path = os.fspath(file_path)\n","    wav,sr = librosa.load(path,sr=sr)\n","    # if wav.shape[0]<5*sr:\n","    #     wav=np.pad(wav,int(np.ceil((5*sr-wav.shape[0])/2)),mode='reflect')\n","    # else:\n","    #     wav=wav[:5*sr]\n","    spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft,\n","                hop_length=hop_length,n_mels=n_mels,fmin=fmin,fmax=fmax)\n","    spec_db=librosa.power_to_db(spec,top_db=top_db)\n","    return spec_db\n","\n","\n","def spec_to_image(spec, eps=1e-6):\n","    mean = spec.mean()\n","    std = spec.std()\n","    spec_norm = (spec - mean) / (std + eps)\n","    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n","    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n","    spec_scaled = spec_scaled.astype(np.uint8)\n","    return spec_scaled"],"metadata":{"id":"wAOgV3kxGdvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["audio_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/audio/'\n","mel_spectrograms_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/mel_spectrograms/'\n","\n","train_dir = mel_spectrograms_dir + 'train'\n","valid_dir = mel_spectrograms_dir + 'validation'"],"metadata":{"id":"Ng5p8wYcYMu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for subdir, dirs, files in os.walk(audio_dir):\n","    if len(files) == 0: \n","        continue\n","    accent = os.path.basename(subdir)\n","    train, test = train_test_split(files, test_size=0.2)\n","    for audio_file in train:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          melspectrogram = spec_to_image(get_melspectrogram_db(audio_path))\n","        except:\n","            continue\n","        img = Image.fromarray(melspectrogram)\n","        save_path = train_dir + '/' + accent\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path)\n","        img.save(save_path+'/'+os.path.splitext(os.path.basename(audio_path))[0]+'.png')\n","\n","    for audio_file in test:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          melspectrogram = spec_to_image(get_melspectrogram_db(audio_path))\n","        except:\n","            continue\n","        img = Image.fromarray(melspectrogram)\n","        save_path = valid_dir + '/' + accent\n","        if not os.path.exists(save_path):\n","            os.makedirs(save_path)\n","        img.save(save_path+'/'+os.path.splitext(os.path.basename(audio_path))[0]+'.png')"],"metadata":{"id":"7qGS5Qgf-arB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Resnet fine-tuning"],"metadata":{"id":"l5fQQ4SBI6Fl"}},{"cell_type":"markdown","source":["## Data Loading"],"metadata":{"id":"PIlyaiD8nn3D"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset\n","import torchaudio\n","\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","\n","import torchvision.models as models"],"metadata":{"id":"9arBNb2EFZUV","executionInfo":{"status":"ok","timestamp":1652917641155,"user_tz":-180,"elapsed":3201,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import os\n","from os import listdir\n","import matplotlib.pyplot as plt\n","from os.path import isfile, join\n","from sklearn.preprocessing import LabelEncoder  \n","import librosa\n","import librosa.display"],"metadata":{"id":"F0hJhFJXoIFl","executionInfo":{"status":"ok","timestamp":1652917641156,"user_tz":-180,"elapsed":13,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# input_size = 224\n","batch_size = 32"],"metadata":{"id":"sreTI7uxJQlI","executionInfo":{"status":"ok","timestamp":1652917641156,"user_tz":-180,"elapsed":11,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"X2US4T13nXu5","executionInfo":{"status":"ok","timestamp":1652917641157,"user_tz":-180,"elapsed":11,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["audio_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/audio/'"],"metadata":{"id":"mZi7j4rLndsn","executionInfo":{"status":"ok","timestamp":1652917641157,"user_tz":-180,"elapsed":10,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["speakers_train_paths = []\n","speakers_train_labels = []\n","\n","speakers_test_paths = []\n","speakers_test_labels = []"],"metadata":{"id":"sCSsatmxnYtQ","executionInfo":{"status":"ok","timestamp":1652917641899,"user_tz":-180,"elapsed":2,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["ban = ['australia', 'canada', 'japanese']\n","for subdir, dirs, files in os.walk(audio_dir):\n","    if len(files) == 0: \n","        continue\n","    accent = os.path.basename(subdir)\n","    if accent in ban:\n","        continue\n","    train, test = train_test_split(files, test_size=0.2)\n","    for audio_file in train:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          wav, sr = librosa.load(audio_path,sr=None)\n","          speakers_train_paths.append(audio_path)\n","          speakers_train_labels.append(accent)\n","        except:\n","            continue\n","\n","    for audio_file in test:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          wav, sr = librosa.load(audio_path,sr=None)\n","          speakers_test_paths.append(audio_path)\n","          speakers_test_labels.append(accent)\n","        except:\n","            continue"],"metadata":{"id":"RYEnaBXCngBZ","executionInfo":{"status":"ok","timestamp":1652917754135,"user_tz":-180,"elapsed":108697,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["le = LabelEncoder()\n","speakers_train_labels = le.fit_transform(speakers_train_labels)\n","speakers_test_labels = le.transform(speakers_test_labels)"],"metadata":{"id":"LfqgNltQnmZu","executionInfo":{"status":"ok","timestamp":1652917754135,"user_tz":-180,"elapsed":14,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["unique_elements_train, counts_elements_train = np.unique(le.inverse_transform(speakers_train_labels), \n","                                                         return_counts=True)\n","\n","unique_elements_test, counts_elements_test = np.unique(le.inverse_transform(speakers_test_labels), \n","                                                       return_counts=True)\n","\n","df = pd.DataFrame({\n","    'Accent': unique_elements_train,\n","    'Train set': counts_elements_train,\n","    'Test set': counts_elements_test\n","})\n","  \n","ax = df.plot(x=\"Accent\", y='Train set', kind=\"bar\", figsize=(14, 6), color=\"lightskyblue\")\n","df.plot(x=\"Accent\", y='Test set', kind=\"bar\", ax=ax, color=\"mediumslateblue\")\n","plt.title('Accents dataset')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"3RZ4f31c-eed","executionInfo":{"status":"ok","timestamp":1652917754135,"user_tz":-180,"elapsed":13,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"2e138900-edeb-495f-d3d3-0718c150c866"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1008x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, audio_paths, labels):\n","        self.audio_paths = audio_paths\n","        self.labels = labels\n","        # self.is_valid = is_valid\n","        # if self.is_valid == 1:\n","        #     self.aug = # transfoms for validation images\n","        # else:                  \n","        #     self.aug = # transfoms for training images\n","\n","    def _wav2fbank(self, filename):\n","        waveform, sr = torchaudio.load(filename)\n","        waveform = waveform - waveform.mean()\n","        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n","                                                 window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10)\n","\n","        target_length = 256\n","        n_frames = fbank.shape[0]\n","\n","        p = target_length - n_frames\n","\n","        if p > 0:\n","            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n","            fbank = m(fbank)\n","        elif p < 0:\n","            fbank = fbank[0:target_length, :]\n","        return fbank\n","            \n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        filename = self.audio_paths[index]\n","        label_indices = self.labels[index]\n","        fbank = self._wav2fbank(filename)\n","\n","        fbank = fbank.squeeze(0)\n","        fbank = torch.transpose(fbank, 0, 1)\n","        norm_mean = -6.757296\n","        norm_std = 4.3780417\n","        fbank = (fbank - norm_mean) / (norm_std * 2)\n","        fbank = torch.reshape(fbank, (1, 128, 256))\n","        fbank = fbank.expand(3, -1, -1)\n","        # label_indices = torch.FloatTensor(label_indices)\n","        return fbank, label_indices\n","\n","train_dataset = MyDataset(speakers_train_paths, speakers_train_labels)\n","test_dataset = MyDataset(speakers_test_paths, speakers_test_labels)"],"metadata":{"id":"ThEkPKbpm53g","executionInfo":{"status":"ok","timestamp":1652917856693,"user_tz":-180,"elapsed":268,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = True)"],"metadata":{"id":"sISusigCm_h_","executionInfo":{"status":"ok","timestamp":1652917856992,"user_tz":-180,"elapsed":2,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["dataloaders_dict = {}\n","dataloaders_dict['train'] = train_loader\n","dataloaders_dict['validation'] = test_loader"],"metadata":{"id":"Shd5wFAFnGOR","executionInfo":{"status":"ok","timestamp":1652917858729,"user_tz":-180,"elapsed":1,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# mean=[]\n","# std=[]\n","# for i, (audio_input, labels) in enumerate(train_loader):\n","#     cur_mean = torch.mean(audio_input)\n","#     cur_std = torch.std(audio_input)\n","#     mean.append(cur_mean)\n","#     std.append(cur_std)\n","#     print(cur_mean, cur_std)\n","# print(np.mean(mean), np.mean(std))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilcQ3O-aoFC8","executionInfo":{"status":"ok","timestamp":1652917810575,"user_tz":-180,"elapsed":56452,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"d27e1fab-5f7b-45ec-890f-1c4efeccb2b8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-6.9430) tensor(4.3433)\n","tensor(-6.8455) tensor(4.5177)\n","tensor(-7.4912) tensor(4.3532)\n","tensor(-6.6834) tensor(4.4257)\n","tensor(-6.6167) tensor(4.4717)\n","tensor(-6.8855) tensor(4.3244)\n","tensor(-6.3951) tensor(4.0690)\n","tensor(-6.4353) tensor(4.3093)\n","tensor(-6.8338) tensor(4.4711)\n","tensor(-6.8109) tensor(4.3764)\n","tensor(-6.2849) tensor(4.1857)\n","tensor(-6.6923) tensor(4.3278)\n","tensor(-6.5970) tensor(4.3776)\n","tensor(-6.5700) tensor(4.2566)\n","tensor(-6.8078) tensor(4.4495)\n","tensor(-6.8852) tensor(4.1533)\n","tensor(-6.9340) tensor(4.1905)\n","tensor(-6.9359) tensor(4.5609)\n","tensor(-6.3158) tensor(4.2634)\n","tensor(-7.1697) tensor(4.4570)\n","tensor(-6.8247) tensor(4.6292)\n","tensor(-6.9538) tensor(4.4126)\n","tensor(-6.8183) tensor(4.3098)\n","tensor(-6.9074) tensor(4.5530)\n","tensor(-6.5337) tensor(4.2833)\n","tensor(-6.9763) tensor(4.2872)\n","tensor(-6.4060) tensor(4.4559)\n","tensor(-6.6168) tensor(4.4099)\n","tensor(-6.9023) tensor(4.3767)\n","tensor(-6.6344) tensor(4.3725)\n","tensor(-6.5805) tensor(4.5154)\n","tensor(-7.1317) tensor(4.5579)\n","tensor(-6.5719) tensor(4.4279)\n","-6.757296 4.3780417\n"]}]},{"cell_type":"code","source":["# data_transforms = {\n","#     'train': transforms.Compose([\n","#         transforms.RandomResizedCrop(input_size),\n","#         # transforms.RandomHorizontalFlip(),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","#     ]),\n","#     'validation': transforms.Compose([\n","#         transforms.Resize(input_size),\n","#         transforms.CenterCrop(input_size),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","#     ]),\n","# }\n","\n","# mel_spectrograms_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/mel_spectrograms/'\n","# image_datasets = {x: ImageFolder(os.path.join(mel_spectrograms_dir, x), data_transforms[x]) for x in ['train', 'validation']}"],"metadata":{"id":"mwvS0KiBKjeO","executionInfo":{"status":"ok","timestamp":1652811148347,"user_tz":-180,"elapsed":7608,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def make_weights_for_balanced_classes(images, nclasses):                        \n","    count = [0] * nclasses                                                      \n","    for item in images:                                                         \n","        count[item[1]] += 1                                                     \n","    weight_per_class = [0.] * nclasses                                      \n","    N = float(sum(count))                                                   \n","    for i in range(nclasses):                                                   \n","        weight_per_class[i] = N/float(count[i])                                 \n","    weight = [0] * len(images)                                              \n","    for idx, val in enumerate(images):                                          \n","        weight[idx] = weight_per_class[val[1]]                                  \n","    return weight                         "],"metadata":{"id":"kxESM7o6YGQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weights = make_weights_for_balanced_classes(image_datasets['train'].imgs, len(image_datasets['train'].classes))                                                                \n","# weights = torch.DoubleTensor(weights)                                       \n","# sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) "],"metadata":{"id":"QxspKCU5XEYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloaders_dict = {}\n","# dataloaders_dict['train'] = torch.utils.data.DataLoader(image_datasets['train'], \n","#                                                    batch_size=batch_size, \n","#                                                   #  shuffle=True, \n","#                                                    sampler=sampler,\n","#                                                    num_workers=4) "],"metadata":{"id":"Sexc2S2qYWJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloaders_dict['validation'] = torch.utils.data.DataLoader(image_datasets['validation'], \n","#                                                    batch_size=batch_size, \n","#                                                    shuffle=True, \n","#                                                    num_workers=4) "],"metadata":{"id":"JGpYiMgRYm-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n","#                                                    batch_size=batch_size, \n","#                                                    shuffle=True, \n","#                                                    num_workers=4) for x in ['train', 'validation']}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaDTNFxdXG1d","executionInfo":{"status":"ok","timestamp":1652811148668,"user_tz":-180,"elapsed":2,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"fac62aa4-a2c3-4b27-fd27-6dbaab581847"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"markdown","source":["## Train & Evaluate"],"metadata":{"id":"U4Pj7nJontxw"}},{"cell_type":"code","source":["num_classes = 7\n","num_epochs = 5"],"metadata":{"id":"cG8TrenYK398","executionInfo":{"status":"ok","timestamp":1652917874877,"user_tz":-180,"elapsed":263,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["resnet18_pretrained = models.resnet18(pretrained=True)\n","resnet18_pretrained.fc = nn.Linear(512, num_classes)"],"metadata":{"id":"FPB3XhTnLtnV","executionInfo":{"status":"ok","timestamp":1652917877092,"user_tz":-180,"elapsed":798,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["73e52f83ae9e4e67b13b878d4a02e7e9","3c2b0638d3054bc2bcdaec18c90bd7e6","809c573f825e4e2cbe0b930a0d4c562c","73129ce29adc4581a7c20363aa50424a","abdc0468edc6432d96ff6e34f3351c39","8401911193e640cc8f2d9626ecb6240b","655d2d2300c246d48631eedcfe50f64e","77ec3edb664849b19ea4aface52b2666","1e5e3942f4fd4779a532d17ecffb4236","48257e9a56654b39a2ecf4ae417e6686","03ff015243904ec380b84ebb44651065"]},"outputId":"21837ca1-fbf7-4a51-99e6-5dfd214d61b6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/44.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e52f83ae9e4e67b13b878d4a02e7e9"}},"metadata":{}}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kUxNgYEL8PR","executionInfo":{"status":"ok","timestamp":1652917879876,"user_tz":-180,"elapsed":301,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"4fbd891c-0c5f-4493-a942-d06abf960fa1"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'validation']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                # try:\n","                #     inputs = inputs.reshape((10, 128, 128))\n","                # except:\n","                #     continue\n","                \n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    outputs = model(inputs)\n","                    \n","                    y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n","                    _, preds = torch.max(y_pred_softmax, dim = 1)    \n","                    loss = criterion(outputs, labels)\n","                    # loss = criterion(outputs, labels)\n","\n","                    # _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'validation' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'validation':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"metadata":{"id":"1Urx0kmrLvp2","executionInfo":{"status":"ok","timestamp":1652917886513,"user_tz":-180,"elapsed":257,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["resnet18_pretrained = resnet18_pretrained.to(device)\n","\n","params_to_update = resnet18_pretrained.parameters()\n","optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)"],"metadata":{"id":"uZ9nVFpDMEoK","executionInfo":{"status":"ok","timestamp":1652917898516,"user_tz":-180,"elapsed":10879,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","# Train and evaluate\n","resnet18_pretrained, hist = train_model(resnet18_pretrained,\n","                             dataloaders_dict,\n","                             criterion, optimizer_ft, \n","                             num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSfsTs2_MmNk","executionInfo":{"status":"ok","timestamp":1652918269325,"user_tz":-180,"elapsed":366292,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"e0232c83-2677-44e0-cf84-3ed0a9b2cd68"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/4\n","----------\n","train Loss: 1.9422 Acc: 0.2329\n","validation Loss: 1.9152 Acc: 0.3195\n","\n","Epoch 1/4\n","----------\n","train Loss: 1.7420 Acc: 0.3165\n","validation Loss: 1.7648 Acc: 0.3120\n","\n","Epoch 2/4\n","----------\n","train Loss: 1.6624 Acc: 0.3717\n","validation Loss: 1.7468 Acc: 0.3421\n","\n","Epoch 3/4\n","----------\n","train Loss: 1.6081 Acc: 0.3945\n","validation Loss: 1.7284 Acc: 0.3383\n","\n","Epoch 4/4\n","----------\n","train Loss: 1.5485 Acc: 0.4259\n","validation Loss: 1.7097 Acc: 0.3459\n","\n","Training complete in 6m 6s\n","Best val Acc: 0.345865\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","from itertools import chain\n","\n","y_pred_list = []\n","y_labels_list = []\n","resnet18_pretrained.eval()\n","with torch.no_grad():\n","    for batch in dataloaders_dict['validation']:\n","        images, labels = batch\n","        y_labels_list.append(labels.numpy())\n","        images, _ = images.cuda(), labels.cuda()\n","        y_test_pred = resnet18_pretrained(images)\n","        y_pred_softmax = torch.log_softmax(y_test_pred, dim = 1)\n","        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) \n","        y_pred_list.append([x for x in y_pred_tags.cpu().numpy()])\n","\n","y_pred_list = list(chain.from_iterable(y_pred_list))\n","y_labels_list = list(chain.from_iterable(y_labels_list))"],"metadata":{"id":"oDWSygtzMpRw","executionInfo":{"status":"ok","timestamp":1652918283876,"user_tz":-180,"elapsed":14566,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_labels_list, y_pred_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YEdFVc6M32O","executionInfo":{"status":"ok","timestamp":1652918283878,"user_tz":-180,"elapsed":29,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"d64f9b5b-2a07-4fd3-8f6e-081dbcc3d83a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.19      0.07      0.11        40\n","           1       0.26      0.14      0.18        43\n","           2       0.00      0.00      0.00        20\n","           3       0.00      0.00      0.00        17\n","           4       0.18      0.19      0.19        47\n","           5       0.00      0.00      0.00        14\n","           6       0.42      0.87      0.56        85\n","\n","    accuracy                           0.35       266\n","   macro avg       0.15      0.18      0.15       266\n","weighted avg       0.24      0.35      0.26       266\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["confusion_matrix(y_labels_list, y_pred_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4v-fCHu1ON_V","executionInfo":{"status":"ok","timestamp":1652918283879,"user_tz":-180,"elapsed":27,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"c1bba3ed-199d-44cf-eb25-450c11a6f953"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3,  5,  0,  0, 16,  0, 16],\n","       [ 3,  6,  0,  0,  5,  0, 29],\n","       [ 2,  3,  0,  0,  8,  0,  7],\n","       [ 1,  2,  0,  0,  3,  0, 11],\n","       [ 4,  3,  0,  0,  9,  0, 31],\n","       [ 1,  1,  0,  0,  3,  0,  9],\n","       [ 2,  3,  0,  0,  6,  0, 74]])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["le.classes_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asiZtSPaOZGV","executionInfo":{"status":"ok","timestamp":1652918343598,"user_tz":-180,"elapsed":288,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"0e1a0421-41e4-4aaa-eaec-3828d8869d33"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['arabic', 'chinese', 'korean', 'russian', 'spanish', 'uk', 'usa'],\n","      dtype='<U7')"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["# Transformer fine-tuning"],"metadata":{"id":"esRAKfZAXoM_"}},{"cell_type":"markdown","source":["## Model loading"],"metadata":{"id":"wj1YN7Zg1vkF"}},{"cell_type":"code","source":["pip install timm==0.4.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtzoWjlOpGns","executionInfo":{"status":"ok","timestamp":1652918416093,"user_tz":-180,"elapsed":3547,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"9433196e-b49a-452f-9b2c-73c9ec5047d2"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm==0.4.5\n","  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n","\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 184 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 204 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 215 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 235 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 245 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 256 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 266 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 276 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 287 kB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.5) (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.5) (0.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.5) (4.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.5) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.5) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.5) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.4.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.4.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.4.5) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.4.5) (2.10)\n","Installing collected packages: timm\n","Successfully installed timm-0.4.5\n"]}]},{"cell_type":"code","source":["from torch.cuda.amp import autocast\n","import os\n","import timm\n","from timm.models.layers import to_2tuple,trunc_normal_\n"],"metadata":{"id":"J7vry1O3o-tC","executionInfo":{"status":"ok","timestamp":1652918416511,"user_tz":-180,"elapsed":421,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# override the timm package to relax the input shape constraint.\n","class PatchEmbed(nn.Module):\n","    def __init__(self, img_size=256, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = (256, 128)\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","\n","class ASTModel(nn.Module):\n","    \"\"\"\n","    The AST model.\n","    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n","    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n","    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n","    :param input_fdim: the number of frequency bins of the input spectrogram\n","    :param input_tdim: the number of time frames of the input spectrogram\n","    :param imagenet_pretrain: if use ImageNet pretrained model\n","    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n","    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n","    \"\"\"\n","    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n","\n","        super(ASTModel, self).__init__()\n","        # assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n","\n","        if verbose == True:\n","            print('---------------AST Model Summary---------------')\n","            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n","        # override timm input shape restriction\n","        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n","\n","        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n","        if audioset_pretrain == False:\n","            if model_size == 'tiny224':\n","                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'small224':\n","                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'base224':\n","                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n","            elif model_size == 'base384':\n","                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n","            else:\n","                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n","            self.original_num_patches = self.v.patch_embed.num_patches\n","            self.oringal_hw = int(self.original_num_patches ** 0.5)\n","            self.original_embedding_dim = self.v.pos_embed.shape[2]\n","            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n","\n","            # automatcially get the intermediate shape\n","            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n","            num_patches = f_dim * t_dim\n","            self.v.patch_embed.num_patches = num_patches\n","            if verbose == True:\n","                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n","                print('number of patches={:d}'.format(num_patches))\n","\n","            # the linear projection layer\n","            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n","            if imagenet_pretrain == True:\n","                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n","                new_proj.bias = self.v.patch_embed.proj.bias\n","            self.v.patch_embed.proj = new_proj\n","\n","            # the positional embedding\n","            if imagenet_pretrain == True:\n","                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n","                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n","                # cut (from middle) or interpolate the second dimension of the positional embedding\n","                if t_dim <= self.oringal_hw:\n","                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n","                else:\n","                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n","                # cut (from middle) or interpolate the first dimension of the positional embedding\n","                if f_dim <= self.oringal_hw:\n","                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n","                else:\n","                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n","                # flatten the positional embedding\n","                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n","                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n","                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n","            else:\n","                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n","                # TODO can use sinusoidal positional embedding instead\n","                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n","                self.v.pos_embed = new_pos_embed\n","                trunc_normal_(self.v.pos_embed, std=.02)\n","\n","        # now load a model that is pretrained on both ImageNet and AudioSet\n","        elif audioset_pretrain == True:\n","            if audioset_pretrain == True and imagenet_pretrain == False:\n","                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n","            if model_size != 'base384':\n","                raise ValueError('currently only has base384 AudioSet pretrained model.')\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            # if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n","                # this model performs 0.4593 mAP on the audioset eval set\n","            # audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n","            # wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n","            \n","            sd = torch.load('/content/drive/MyDrive/NIS_2022/accent_archive/audioset_10_10_0.4495.pth', map_location=device)\n","            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, \n","                                   imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n","            audio_model = torch.nn.DataParallel(audio_model)\n","            audio_model.load_state_dict(sd, strict=False)\n","            self.v = audio_model.module.v\n","            self.original_embedding_dim = self.v.pos_embed.shape[2]\n","            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n","\n","            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n","            num_patches = f_dim * t_dim\n","            self.v.patch_embed.num_patches = num_patches\n","            if verbose == True:\n","                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n","                print('number of patches={:d}'.format(num_patches))\n","\n","            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n","            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n","            if t_dim < 101:\n","                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n","            # otherwise interpolate\n","            else:\n","                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n","            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n","            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n","\n","    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n","        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n","        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n","        test_out = test_proj(test_input)\n","        f_dim = test_out.shape[2]\n","        t_dim = test_out.shape[3]\n","        return f_dim, t_dim\n","\n","    @autocast()\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n","        :return: prediction\n","        \"\"\"\n","        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n","        x = x.unsqueeze(1)\n","        x = x.transpose(2, 3)\n","\n","        B = x.shape[0]\n","        x = self.v.patch_embed(x)\n","        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n","        dist_token = self.v.dist_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n","        x = x + self.v.pos_embed\n","        x = self.v.pos_drop(x)\n","        for blk in self.v.blocks:\n","            x = blk(x)\n","        x = self.v.norm(x)\n","        x = (x[:, 0] + x[:, 1]) / 2\n","\n","        x = self.mlp_head(x)\n","        return x"],"metadata":{"id":"TmVaw3G_oqpF","executionInfo":{"status":"ok","timestamp":1652919973358,"user_tz":-180,"elapsed":409,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["pretrained_mdl_path = '/content/drive/MyDrive/NIS_2022/accent_archive/audioset_10_10_0.4495.pth'\n","fstride, tstride = int(pretrained_mdl_path.split('/')[-1].split('_')[1]), int(pretrained_mdl_path.split('/')[-1].split('_')[2].split('.')[0])\n","input_tdim = 128\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","audio_model = ASTModel(imagenet_pretrain=True, audioset_pretrain=True,\n","                       input_tdim=input_tdim, label_dim=7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGTt3NfIk54R","executionInfo":{"status":"ok","timestamp":1652919978741,"user_tz":-180,"elapsed":1878,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"ac09629a-0907-4cdc-c846-bebb82ed21af"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------AST Model Summary---------------\n","ImageNet pretraining: True, AudioSet pretraining: True\n","frequncey stride=10, time stride=10\n","number of patches=144\n"]}]},{"cell_type":"code","source":["audio_model = audio_model.to(device)"],"metadata":{"id":"K3Q4QAaB0tn4","executionInfo":{"status":"ok","timestamp":1652812048973,"user_tz":-180,"elapsed":266,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# test_input = torch.rand([16, input_tdim, 128]).to(device)\n","# test_output = audio_model(test_input)"],"metadata":{"id":"iXbzsBhgx57N","executionInfo":{"status":"ok","timestamp":1652738857861,"user_tz":-180,"elapsed":324,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## Train & Evaluate"],"metadata":{"id":"6voOKq7e2DD-"}},{"cell_type":"code","source":["input_size = 128\n","batch_size = 10"],"metadata":{"id":"1QIPUHA11yQ4","executionInfo":{"status":"ok","timestamp":1652812054206,"user_tz":-180,"elapsed":239,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Grayscale(num_output_channels=1),\n","        transforms.RandomResizedCrop(input_size),\n","        # transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'validation': transforms.Compose([\n","        transforms.Grayscale(num_output_channels=1),\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","mel_spectrograms_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/mel_spectrograms/'\n","image_datasets = {x: ImageFolder(os.path.join(mel_spectrograms_dir, x), data_transforms[x]) for x in ['train', 'validation']}"],"metadata":{"id":"E8CNeIqV03sr","executionInfo":{"status":"ok","timestamp":1652812058231,"user_tz":-180,"elapsed":411,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n","                                                   batch_size=batch_size, \n","                                                   shuffle=True, \n","                                                   num_workers=4) for x in ['train', 'validation']}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wi56MB8T1zzV","executionInfo":{"status":"ok","timestamp":1652812059612,"user_tz":-180,"elapsed":292,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"07bb5a4b-b435-4b43-984a-e9d621b54d4a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["params_to_update = [p for p in audio_model.parameters() if p.requires_grad]\n","\n","optimizer = torch.optim.Adam(params_to_update, 0.001, weight_decay=5e-7, betas=(0.95, 0.999))\n","# optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"],"metadata":{"id":"7TZBgnuG2c8r","executionInfo":{"status":"ok","timestamp":1652731247292,"user_tz":-180,"elapsed":269,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10"],"metadata":{"id":"l0s3N5tc37_C","executionInfo":{"status":"ok","timestamp":1652731247600,"user_tz":-180,"elapsed":4,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","# Train and evaluate\n","audio_model, hist = train_model(audio_model,\n","                                dataloaders_dict,\n","                                criterion, optimizer, \n","                                num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lW29F3BM3sWU","executionInfo":{"status":"ok","timestamp":1652731399596,"user_tz":-180,"elapsed":151022,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"5e33567a-e6a2-4164-96b0-f1e08f1ab8b5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/9\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.2603 Acc: 0.2312\n","validation Loss: 2.0668 Acc: 0.2891\n","\n","Epoch 1/9\n","----------\n","train Loss: 2.0573 Acc: 0.2823\n","validation Loss: 2.0271 Acc: 0.2891\n","\n","Epoch 2/9\n","----------\n","train Loss: 2.0349 Acc: 0.2909\n","validation Loss: 2.0093 Acc: 0.2959\n","\n","Epoch 3/9\n","----------\n","train Loss: 2.0030 Acc: 0.2909\n","validation Loss: 2.0170 Acc: 0.2823\n","\n","Epoch 4/9\n","----------\n","train Loss: 2.0167 Acc: 0.2814\n","validation Loss: 2.0113 Acc: 0.2381\n","\n","Epoch 5/9\n","----------\n","train Loss: 1.9820 Acc: 0.2909\n","validation Loss: 1.9981 Acc: 0.2925\n","\n","Epoch 6/9\n","----------\n","train Loss: 1.9747 Acc: 0.2961\n","validation Loss: 2.0597 Acc: 0.2789\n","\n","Epoch 7/9\n","----------\n","train Loss: 1.9927 Acc: 0.2745\n","validation Loss: 2.0016 Acc: 0.2517\n","\n","Epoch 8/9\n","----------\n","train Loss: 1.9898 Acc: 0.3074\n","validation Loss: 1.9942 Acc: 0.2585\n","\n","Epoch 9/9\n","----------\n","train Loss: 1.9602 Acc: 0.3091\n","validation Loss: 2.0414 Acc: 0.2449\n","\n","Training complete in 2m 31s\n","Best val Acc: 0.295918\n"]}]},{"cell_type":"markdown","source":["# Transformer fine-tuning + ast data prepocessing"],"metadata":{"id":"BAkb34x42KZ6"}},{"cell_type":"markdown","source":["## Data loading"],"metadata":{"id":"51D-r2Jm2aqN"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"yrLTJObSF9I3","executionInfo":{"status":"ok","timestamp":1652741922472,"user_tz":-180,"elapsed":287,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["audio_dir = '/content/drive/MyDrive/NIS_2022/accent_archive/data/audio/'"],"metadata":{"id":"n2eb1UFlmeDh","executionInfo":{"status":"ok","timestamp":1652812077969,"user_tz":-180,"elapsed":384,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["speakers_train_paths = []\n","speakers_train_labels = []\n","\n","speakers_test_paths = []\n","speakers_test_labels = []"],"metadata":{"id":"Dglxgf2zmioN","executionInfo":{"status":"ok","timestamp":1652812078320,"user_tz":-180,"elapsed":1,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["for subdir, dirs, files in os.walk(audio_dir):\n","    if len(files) == 0: \n","        continue\n","    accent = os.path.basename(subdir)\n","    train, test = train_test_split(files, test_size=0.2)\n","    for audio_file in train:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          wav, sr = librosa.load(audio_path,sr=None)\n","          speakers_train_paths.append(audio_path)\n","          speakers_train_labels.append(accent)\n","        except:\n","            continue\n","\n","    for audio_file in test:\n","        audio_path = os.path.join(subdir, audio_file)\n","        try:\n","          wav, sr = librosa.load(audio_path,sr=None)\n","          speakers_test_paths.append(audio_path)\n","          speakers_test_labels.append(accent)\n","        except:\n","            continue"],"metadata":{"id":"70zUVIGwmREi","executionInfo":{"status":"ok","timestamp":1652812227326,"user_tz":-180,"elapsed":147001,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["speaker_set = set(speakers_test_labels)\n","number_of_unique_values = len(speaker_set)\n","print(speaker_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yKvySRGpncv","executionInfo":{"status":"ok","timestamp":1652812227326,"user_tz":-180,"elapsed":33,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"0b492249-e64d-4fb9-8cbc-161b3d4d3f8a"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["{'uk', 'canada', 'spanish', 'korean', 'japanese', 'arabic', 'chinese', 'usa', 'australia', 'russian'}\n"]}]},{"cell_type":"code","source":["le = LabelEncoder()\n","speakers_train_labels = le.fit_transform(speakers_train_labels)\n","speakers_test_labels = le.transform(speakers_test_labels)"],"metadata":{"id":"jhH8ibTlpy6d","executionInfo":{"status":"ok","timestamp":1652812227327,"user_tz":-180,"elapsed":28,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["speaker_set = set(speakers_test_labels)\n","number_of_unique_values = len(speaker_set)\n","print(speaker_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3IjcbPRdsJSe","executionInfo":{"status":"ok","timestamp":1652812227327,"user_tz":-180,"elapsed":27,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"7932e69e-4ded-48d2-c86c-b6e10d687bbc"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"]}]},{"cell_type":"code","source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, audio_paths, labels):\n","        self.audio_paths = audio_paths\n","        self.labels = labels\n","        # self.is_valid = is_valid\n","        # if self.is_valid == 1:\n","        #     self.aug = # transfoms for validation images\n","        # else:                  \n","        #     self.aug = # transfoms for training images\n","\n","    def _wav2fbank(self, filename):\n","        waveform, sr = torchaudio.load(filename)\n","        waveform = waveform - waveform.mean()\n","        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n","                                                 window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10)\n","\n","        target_length = 128\n","        n_frames = fbank.shape[0]\n","\n","        p = target_length - n_frames\n","\n","        if p > 0:\n","            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n","            fbank = m(fbank)\n","        elif p < 0:\n","            fbank = fbank[0:target_length, :]\n","        return fbank\n","            \n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        filename = self.audio_paths[index]\n","        label_indices = self.labels[index]\n","        fbank = self._wav2fbank(filename)\n","\n","        fbank = fbank.squeeze(0)\n","        fbank = torch.transpose(fbank, 0, 1)\n","        norm_mean = -6.757296\n","        norm_std = 4.3780417\n","        fbank = (fbank - norm_mean) / (norm_std * 2)\n","\n","        # label_indices = torch.FloatTensor(label_indices)\n","        return fbank, label_indices\n","\n","train_dataset = MyDataset(speakers_train_paths, speakers_train_labels)\n","test_dataset = MyDataset(speakers_test_paths, speakers_test_labels)"],"metadata":{"id":"r-QdP7YMgXAI","executionInfo":{"status":"ok","timestamp":1652918370586,"user_tz":-180,"elapsed":337,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = True)"],"metadata":{"id":"wuUekeIonoPF","executionInfo":{"status":"ok","timestamp":1652918379243,"user_tz":-180,"elapsed":280,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["dataloaders_dict = {}\n","dataloaders_dict['train'] = train_loader\n","dataloaders_dict['validation'] = test_loader"],"metadata":{"id":"h8Ovhhp5qVRn","executionInfo":{"status":"ok","timestamp":1652918380035,"user_tz":-180,"elapsed":4,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## Model loading"],"metadata":{"id":"2X4auDUC2hUZ"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"JQJxszm7o2rk","executionInfo":{"status":"ok","timestamp":1652918383146,"user_tz":-180,"elapsed":294,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["pretrained_mdl_path = '/content/drive/MyDrive/NIS_2022/accent_archive/audioset_10_10_0.4495.pth'\n","fstride, tstride = int(pretrained_mdl_path.split('/')[-1].split('_')[1]), int(pretrained_mdl_path.split('/')[-1].split('_')[2].split('.')[0])\n","input_tdim = 128\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","audio_model = ASTModel(imagenet_pretrain=True, audioset_pretrain=True,\n","                       input_tdim=input_tdim, label_dim=7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRjCeGlboicA","executionInfo":{"status":"ok","timestamp":1652920008893,"user_tz":-180,"elapsed":1988,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"aa88672b-bbb7-46c8-f993-6014b6750c23"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------AST Model Summary---------------\n","ImageNet pretraining: True, AudioSet pretraining: True\n","frequncey stride=10, time stride=10\n","number of patches=144\n"]}]},{"cell_type":"code","source":["audio_model = audio_model.to(device)"],"metadata":{"id":"gnGBuTgTolCE","executionInfo":{"status":"ok","timestamp":1652920008894,"user_tz":-180,"elapsed":5,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["## Train & Evaluate"],"metadata":{"id":"DGYqrJtF2qvW"}},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'validation']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                # try:\n","                #     inputs = inputs.reshape((10, 128, 128))\n","                # except:\n","                #     continue\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    \n","                    y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n","                    _, preds = torch.max(y_pred_softmax, dim = 1)    \n","                    \n","                    # _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'validation' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'validation':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"metadata":{"id":"w_noWezQv4-v","executionInfo":{"status":"ok","timestamp":1652920014952,"user_tz":-180,"elapsed":260,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["params_to_update = [p for p in audio_model.parameters() if p.requires_grad]\n","\n","optimizer = torch.optim.Adam(params_to_update, 0.0001, weight_decay=5e-7, betas=(0.95, 0.999))"],"metadata":{"id":"f2wGSV84pXxR","executionInfo":{"status":"ok","timestamp":1652920019863,"user_tz":-180,"elapsed":282,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# loss_fn = nn.BCEWithLogitsLoss()\n","loss_fn = nn.CrossEntropyLoss()\n","num_epochs = 5"],"metadata":{"id":"Kyy-HFKprAFI","executionInfo":{"status":"ok","timestamp":1652920024269,"user_tz":-180,"elapsed":301,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["audio_model, hist = train_model(audio_model,\n","                                dataloaders_dict,\n","                                loss_fn, optimizer, \n","                                num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzbCf_o1pZ2L","executionInfo":{"status":"ok","timestamp":1652920458894,"user_tz":-180,"elapsed":434279,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"10d4ee58-ac7d-4a80-d71e-1c62d3b22fdd"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/4\n","----------\n","train Loss: 1.9920 Acc: 0.3327\n","validation Loss: 1.8267 Acc: 0.3195\n","\n","Epoch 1/4\n","----------\n","train Loss: 1.7446 Acc: 0.3593\n","validation Loss: 1.7417 Acc: 0.3609\n","\n","Epoch 2/4\n","----------\n","train Loss: 1.6875 Acc: 0.3574\n","validation Loss: 1.7409 Acc: 0.3571\n","\n","Epoch 3/4\n","----------\n","train Loss: 1.6863 Acc: 0.3584\n","validation Loss: 1.7502 Acc: 0.3496\n","\n","Epoch 4/4\n","----------\n","train Loss: 1.6948 Acc: 0.3612\n","validation Loss: 1.7424 Acc: 0.3571\n","\n","Training complete in 7m 14s\n","Best val Acc: 0.360902\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","from itertools import chain\n","\n","y_pred_list = []\n","y_labels_list = []\n","audio_model.eval()\n","with torch.no_grad():\n","    for batch in dataloaders_dict['validation']:\n","        images, labels = batch\n","        y_labels_list.append(labels.numpy())\n","        images, _ = images.cuda(), labels.cuda()\n","        y_test_pred = audio_model(images)\n","        y_pred_softmax = torch.log_softmax(y_test_pred, dim = 1)\n","        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) \n","        y_pred_list.append([x for x in y_pred_tags.cpu().numpy()])\n","\n","y_pred_list = list(chain.from_iterable(y_pred_list))\n","y_labels_list = list(chain.from_iterable(y_labels_list))"],"metadata":{"id":"y87wVc4hqjnb","executionInfo":{"status":"ok","timestamp":1652920474651,"user_tz":-180,"elapsed":15785,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_labels_list, y_pred_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yvztygMinGQ","executionInfo":{"status":"ok","timestamp":1652920474652,"user_tz":-180,"elapsed":40,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"cfd45934-fec8-4e34-e27f-7f57f063459a"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.23      0.47      0.31        40\n","           1       0.29      0.05      0.08        43\n","           2       0.00      0.00      0.00        20\n","           3       0.00      0.00      0.00        17\n","           4       0.00      0.00      0.00        47\n","           5       0.00      0.00      0.00        14\n","           6       0.43      0.88      0.58        85\n","\n","    accuracy                           0.36       266\n","   macro avg       0.13      0.20      0.14       266\n","weighted avg       0.22      0.36      0.24       266\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["confusion_matrix(y_labels_list, y_pred_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5zdmLyUi8le","executionInfo":{"status":"ok","timestamp":1652920474652,"user_tz":-180,"elapsed":33,"user":{"displayName":"Polina Smolnikova","userId":"01669568308474740016"}},"outputId":"6e8c1625-d9c6-47cb-abcc-93dd8b1ab71e"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[19,  2,  0,  0,  0,  0, 19],\n","       [15,  2,  0,  0,  0,  0, 26],\n","       [11,  0,  0,  0,  0,  0,  9],\n","       [ 8,  0,  0,  0,  0,  0,  9],\n","       [17,  2,  0,  0,  0,  0, 28],\n","       [ 4,  1,  0,  0,  0,  0,  9],\n","       [10,  0,  0,  0,  0,  0, 75]])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":[""],"metadata":{"id":"0KBoabTAmqkC"},"execution_count":null,"outputs":[]}]}